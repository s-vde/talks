<!DOCTYPE html>
<html>
   <head>
      <title>Testing and Verifying Multi-Threaded Programs</title>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
      <link rel="stylesheet" type="text/css" href="talk.css"/>
   </head>
   <body>
      <textarea id="source">





<!-- section:Title -->

class: slide_title, no_number
count: false

# Automated Verification of Multi-Threaded Programs
### Susanne van den Elsen
Software Developer @ Native Instruments

### Meeting C++
November 10, 2017

<!-- TODO INTRODUCTION, NI, research -->

???
My goal in this talk is to first provide an insight into why testing and verifying the correctness 
of multi-threaded programs is challenging, and why traditional testing tools are inadequate for 
meeting those challenges.
Then I will present a technique, called Systematic Exploration, which is designed to overcome 
excatly those challenges.
I will demonstrate Systematic Exploration using a tool that I built in the context of my research on 
the topic.





<!-- section:Challenge In Testing And Verifying Mulit-Threaded Programs -->

<!-- >>> Slide -->

---
class: slide_section, no_number
count: false
layout: false
# Challenges in Testing and Verifying Multi-Threaded Programs

???
Let's look at why testing and verifying multi-threaded programs is challenging.

When testing sequential programs, there are different types of bugs to look out for.
These include errors like:
<!-- TODO -->

All these bugs can occur in multi-threaded programs too..., plus more.
Programming with threads introduces a whole new class of potential bugs, which I will refer to as 
"Concurrency Bugs"


<!-- <<< -->

<!-- subsection:Concurrency Bugs -->

<!-- >>> Slide -->

---
# Concurrency Bugs

<!-- <<< -->

???
BRIDGE ==>:
<!-- TODO: memory errors, and ... -->
Just like there are tools specifically tailored to detect typical memory errors, concurrency bugs
ask for tools that specialize in detecting them. Luckily there are such tools available.

<!-- subsubsection:Concurrency Bug Detectors -->

---
layout: true
#### Challenges in Testing and Verifying Multi-Threaded Programs | Concurrency Bugs
# Concurrency Bug Detectors

<!-- >>> Slide -->

---

???
The most well-known concurrency error detectors are probably ThreadSanitizer and Helgrind.
At a very high level, these tools work as follows:

<!-- <<< -->

<!-- >>> Slide -->

---
count: false
##### ThreadSanitizer / Helgrind

<div class="mermaid" style="align: center; width: 100%; margin: 0px auto; margin-top: 3em;">
   graph LR
      input_program[Source/Binary<br/>Input Program]
      exe_instr[Instrumented Executable]
      input_program --> |Compiler/Binary<br/>Instrumentation| exe_instr
      core[Analysis Core<br/><br/>-  Monitoring facilities<br/><br/>-  Datarace/Deadlock<br/>&nbsp&nbspDetection Algorithms]
      core --> |Execute| exe_instr
      exe_instr --> |Trace| core
      core --> report[Bug Report]
</div>

???
- They instrument the program under test so that it can be monitored by the core of the tool.
- In the case of ThreadSanitizer, this instrumentation happens on the source-code level when 
  compiling, while Helgrind instruments binary input programs.
- The instrumented executable is then executed.
  The instrumentation allows the analysis core to record interesting events in the running program 
  and the order in which they happened. Such a record is also called a "trace" of the execution. 
  Events that are tracked include:
   - thread creation, joining and destruction;
   - memory accesses;
   - lock and unlock instructions
- The analysis core deploys algorithms that are specifically tailored to detect potential dataraces
  and deadlocks in a trace.
- If bugs are found, they are reported to the user.

Tools like ThreadSanitizer and Helgrind have been applied to large C++ programs and have 
successfully found concurrency errors, particularly data races, in real software applications.

Needless to say, if you're programming with threads, I highly encourage you to use these tools if 
you don't already.

What these tools are good at is *detecting* concurrency bugs when they occur in the execution of 
your program that they happended to monitor. However, as we'll see next, whether or not a potential 
bug in the program occurs depends heavily on the actual runtime interleaving of threads.

The following small bank account program illustrates this dependence.

<!-- <<< -->

<!-- subsubsection:Dependence on Thread Interleaving -->

---
layout: true
#### Challenges in Testing and Verifying Multi-Threaded Programs | Concurrency Bugs
# Dependence on Thread Interleaving
<!-- <div class="h_example">bank_account.cpp</div> -->

<!-- example:bank_account.cpp -->

<!-- >>> Slide -->

---
```cpp
struct bank_account { atomic<int> balance; }

void transfer(bank_account& sender, int amount, bank_account& receiver);
```

???
A bank account has a balance and there is a function for transferring a given amount of money from
one bank account (sender) to the other (the receiver).
The specification of the transfer function states that when the function returns, the balances 
of both involved accounts are non-negative.

One test case for the transfer function checks that this is true in case two threads attempt to 
transfer the whole amount of money out of the same account concurrently.

--
count: false
```cpp
TEST(AfterConcurrentTransfersBalancesArePositive)
{
   array<bank_account, 2> accounts = {{ 100, 0 }};
   thread mallory(transfer, ref(accounts[0]), 100, ref(accounts[1]));
   thread marvin(transfer, ref(accounts[0]), 100, ref(accounts[1]));
   
   mallory.join();
   marvin.join();
   
   ASSERT(accounts[0] >= 0 && accounts[1] >= 0);
}
```

???
- There are two accounts, one with an initial balance of 100, and one with an initial balance of 0
- Two adversary threads, Mallory and Marvin both try to transfer 100 euros from the first to the 
  second account.
- Obviously, if the program meets the specification, at least one of them should not succeed
- The assertion states that after both threads are joined, both accounts have non-negative balance.

<!-- <<< -->

<!-- >>> Slide -->

---

```cpp
void transfer(bank_account& sender, int amount, bank_account& receiver)
{
   const auto balance_sender = sender.balance.load();
   if (balance_sender >= amount)
   {
      receiver.balance.fetch_add(amount);
      sender.balance.fetch_sub(amount);
   }
}
```

???
Here is an *attempt* at an implementation of the transfer function.
1. First, the balance of the sender's account is atomically read and stored in a local variable
2. Then, the function checks if that balance is sufficient for transferring the given amount of 
   money out of it
3. If so, it atomically adds the amount to the receiver's account and 
   subsequently subtracts the same amount from the sender's account.

<!-- <<< -->

<!-- >>> Slide -->

--
```cpp
mallory                                         marvin                                          accounts

                                                                                                { 100,  0   }
balance_sender = sender.balance.load() = 100;                                                
receiver.balance.fetch_add(amount);                                                             { 100,  100 }
sender.balance.fetch_sub(amount);                                                               { 0,    100 }
                                                balance_sender = sender.balance.load() = 0;  
```

--
count: false
```cpp
ASSERT(accounts[0] >= 0 && accounts[1] >= 0);   // assertion holds :(
```

<!-- <<< -->

<!-- >>> Slide -->

---

```cpp
void transfer(bank_account& sender, int amount, bank_account& receiver)
{
   const auto balance_sender = sender.balance.load();
   if (balance_sender >= amount)
   {
      receiver.balance.fetch_add(amount);
      sender.balance.fetch_sub(amount);
   }
}
```

```cpp
mallory                                         marvin                                          accounts

                                                                                                { 100,  0   }
balance_sender = sender.balance.load() = 100;                                                 
                                                balance_sender = sender.balance.load() = 100;
                                                receiver.balance.fetch_add(amount);             { 100,  100 }
                                                sender.balance.fetch_sub(amount);               { 0,    100 }
receiver.balance.fetch_add(amount);                                                             { 0,    200 }
sender.balance.fetch_sub(amount);                                                               { -100, 200 }         

```

--
count: false
```cpp
ASSERT(accounts[0] >= 0 && accounts[1] >= 0);   // assertion is violated :)
```

???
Whether or not the executing the unit test reveals the bug depends on the interleaving of the two 
threads at runtime.

This brings me to the second challenge for testing and verifying multi-threaded programs:
From the viewpoint of the programmer, this interleaving is nondeterministic.

<!-- <<< -->

<!-- subsection:Nondeterminism -->

---
layout: true
#### Challenges in Testing and Verifying Multi-Threaded Programs
# Nondeterminism

<!-- >>> Slide -->

---

???
The order in which threads get to execute is *not* under the programmer's control, it is under 
the control of the OS scheduler.

--
count: false
##### OS Scheduler Policy
e.g.

???
The OS scheduler makes it scheduling decisions based on a scheduler policy, which may vary from 
OS to OS.
The policy may prescribe, among other things:

--
count: false
- preemptive / non-preemptive

???
- Whether or not executing threads may be preempted, that is, whether the scheduler may force a 
  context-switch to another thread, even though the running thread still has instructions to 
  execute.

--
count: false
- assignment of priorities

???
- If and how threads get assigned priorities
  
The scheduling decisions are also highly dependent on the execution environment. 
Factors include:

--
count: false
##### Execution Environment
e.g.

--
count: false
- current CPU load

???
- CPU load: may have an effect on whether threads get preempted

--
count: false
- presence and priorities of other threads on the CPU

???
- Prios: A thread on a CPU with many high-priority threads may exerience a slow-down relative to 
  other threads in the same process on a different CPU

--
count: false
- state of the cache

???
- Cache: e.g. is the current thread slowed down by cache misses

<!-- <<< -->

---
layout: true
#### Challenges in Testing and Verifying Multi-Threaded Programs | Nondeterminism
# Implications

<!-- >>> Slide -->

---

???
What are the implications of nondeterminism for testing and verification of multi-threaded programs?

--
count: false
##### Bugs are hard to find:

--
count: false
- Some bugs only occur under very rare thread interleavings
<!-- TODO: Approach of inducing bad schedules by forcing context-switches is not systematic! -->

--
count: false
- Running tests repeatedly does not guarantee that interleavings will be different

--
count: false
- "Heisenbugs" may even disappear when debugging them

???
So-called Heisenbugs: for example as a result of debugging statements in the code, which also may 
have a dramatic effect on the scheduling choices, and may render the erronous interleaving way less 
likely to occur.

--
count: false
##### Bugs are hard to reproduce: 
- If buggy behavior is observed, the next execution may be under a different schedule

???
If you've been lucky enough to find the bug, the bug is typically very hard to reproduce.

???
BRIDGE ==>:
So, nondeterminism poses a serious challenge for testing and verifying multi-threaded programs.

In the next part of the talk I am going to discuss Systematic Exploration, which is a technique to 
overcome exactly that challenge.





<!-- section:Systematic Exploration -->

<!-- >>> Slide -->

---
class: slide_section, no_number
count: false
layout: false
# Systematic Exploration

???
The main idea behind Systematic Exploration is to repeatedly execute the program under test, each 
time with a different schedule from a set of interesting schedules. 

Thereto, the exploration tool first has to gain control over the program's thread interleaving at 
runtime. In my tool, this is taken care of by the RecordReplay Library

<!-- <<< -->

---
layout: true
#### Systematic Exploration

<!-- >>> Slide -->

<!-- <<< -->

---
layout: true
#### Systematic Exploration
# RecordReplay library: Taking "Control" over the Scheduler

<!-- >>> Slide -->

---
<div class="mermaid" style="align: center; width: 60%; margin: 0px auto;">
graph TD
   sut[Souce Code]
   ir[LLVM IR]
   ir_instr[Instrumented IR]
   scheduler[Run Time Scheduler Library]
   exe_instr[Instrumented Executable]
   sut --> |Clang| ir
   ir --> |RecordReplay Instrumentation Pass| ir_instr
   ir_instr --> exe_instr
   scheduler --> exe_instr
</div>

???
The RecordReplay library which has two components: 
- a Scheduler Library and
- a compiler instrumentation pass built on top of the LLVM compiler infrastructure.

- The instrumentation process starts with the program's source code.
- This source code is compiled to LLVM Intermediate Representation using Clang
- It instruments the program's intermediate representation by inserting calls to functions from the 
  Scheduler library, which allow the Scheduler to:
  * control which of the program's threads gets to execute in every scheduling round; and
  * record the execution's trace
  Calls to the scheduler are inserted before thread creation/joining, memory instructions, and 
  lock instructions
- Then the instrumented source code is linked to the Scheduler library into an executable.

<!-- 
NOTE: 
LLVM IR: low-level assembly-like language that is designed so that many higher level langauges can 
be mapped to it;
Choice for LLVM route makes it possible to apply the tool to other languages that have a compiler
frontend that translated to LLVM IR;
-->

---
<div class="mermaid" style="align: left; width: 50%; margin: 0px auto;">
graph LR
   schedule[0.0.0.1.1.0.2.2.0]
   exe_instr[Instrumented Executable]
   trace
   schedule --> exe_instr
   exe_instr --> trace
</div>
<div class="animation_no_shift_up" style="margin-top: -5em;">
   .right[<img src='./generated_trees/trace.png' height="450"/>]
</div>


???
The instrumented executable now takes as input a schedule and after execution dumps a trace of the 
execution.

This trace is used by the Systematic Exploration algorithms implemented in the StateSpaceExplorer 
tool to compute the next schedules to run the instrumented executable under.

Let's take a look at two algorithms implemented in the StateSpaceExplorer tool.

<!-- <<< -->

---
layout: true
#### Systematic Exploration
# Depth-First Exploration

<!-- >>> Slide -->

---

```cpp
depth_first_exploration(program)
{
   for (schedule : possible_schedules(program))
   {
      run(program, schedule);
   }
}
```

???
The first algorithm is a simple depth-first exploration.
It simply explores all possible thread interleavings of the original input program.
The set of possible interleavings is computed on-the-fly.
The next slide illustrates this on the background_thread example I introduced earlier in the talk.

<!-- <<< -->

<!-- >>> Slide -->

---
layout: true
#### Systematic Exploration
# Depth-First Exploration
<span class="h_example">background_thread.cpp</span>

---

---
count: false
<div class="animation_no_shift_up" style="margin-right: 6em; margin-top: -4em;">
   .right[<img src='./generated_trees/background_thread.cpp/depth_first_search/trees/animations/0.png' height="500"/>]
</div>

???
- The exploration starts with a certain schedule, in this case a non-preemptive one that prioritizes
  threads in the order in which they were created.
- The trace contains for each node in the branch the set of threads that are enabled in that state. 
- And the algorithm maintains for each node the set of threads it has already scheduled from that 
  state.
- Then it backtracks the annotated trace until it finds a state in which there is an enabled thread 
  that has not been scheduled from that state yet.
- It appends that thread's id to the schedule up to that state and reexecutes the program under the 
  resulting schedule.

---
count: false 
<div class="animation_no_shift_up" style="margin-right: 6em; margin-top: -4em;">
   .right[<img src='./generated_trees/background_thread.cpp/depth_first_search/trees/animations/1.png' height="500"/>]
</div>



---
count: false 
<div class="animation_no_shift_up" style="margin-right: 6em; margin-top: -4em;">
   .right[<img src='./generated_trees/background_thread.cpp/depth_first_search/trees/animations/2.png' height="500"/>]
</div>
---
count: false 
<div class="animation_no_shift_up" style="margin-right: 6em; margin-top: -4em;">
   .right[<img src='./generated_trees/background_thread.cpp/depth_first_search/trees/animations/3.png' height="500"/>]
</div>

<!-- >>> Slide -->

---
count: false
.center[<img src='./generated_trees/background_thread.cpp/depth_first_search/trees/full_schedules_small.png' width="1100"/>]

???
This process continues, until eventually it reaches the root node of the tree again and all 
enabled threads have been scheduled from that root.

This algorithm is very straightforward and, because it executes the program under all 
possible thread interleavings is guaranteed to find concurrency bugs if they are present.
However: it suffers from a *major* problem: combinatorial explosion!

The number of thread interleavings is exponential in the size of the program.
For the relatively small background_thread program the search tree is still managable, but for 
real-world applications with many threads and many instructions, Depth First Search simply doesn't 
scale.

The solution is to prune the tree of schedules considered by depth-first exploration
and only explore a subset of the possible thread interleavings of an input program.

<!-- <<< -->

<!-- >>> Slide -->

---
layout: true
#### Systematic Exploration
# Prune the State Space

---

--
count: false
##### Challenge
Explore a subset of the possbile interleavings, but still provide coverage guarantees
- Complete coverage: all concurrency bugs in the program are found
- Partial coverage: all concurrency bugs with a certain property are found
   e.g.: 
   - all bugs reachable by a schedule of length 10
   - all bugs reachable by a schedule with at most 2 context-switches
   

???
The challenge is to choose that subset so carefully that coverage guarantees can still be provided.
A coverage guarantee can be complete: it states that ...
Or partial: in that case it should be able to say that all concurrency bugs with a certain property
are found.

StateSpaceExplorer implements several state space pruning algorithms, each providing different 
coverage guarantees. In the rest of this talk I will discuss one of them: Partial Order Reduction.

<!-- <<< -->

---
layout: true
#### Systematic Exploration | Exploration Algorithms
# Partial Order Reduction

<!-- >>> Slide -->

---

???
The idea behind partial order reduction is to group schedules whose resulting executions are 
semantically equivalent, at least with respect to the class of concurrency bugs we're checking the 
input program against.

<!-- TODO: Credits -->

--
count: false
```cpp
partial_order_reduction(program, equivalence_relation)
{
   for (equivalence_class : equivalence_classes(program, equivalence_relation))
   {
      schedule = representative(equivalence_class);
      run(program, schedule);
   }
}
```

???
The exploration algorithm makes sure that by the time it's finished it has, for each such 
equivalence class of schedules, seen at least one (and ideally exactly one) representative 
execution.

BRIDGE FROM:
At the heart of the equivalence relation is the concept of dependence. For verifying against dataraces, deadlocks
and assertion failures the following dependence relation can be used.

<!-- <<< -->

<!-- >>> Slide -->

---

##### Dependence Relation
Two instructions are *dependent* iff
- they are carried out by the same thread; or
- they operate on the same operand and at least one of them is a write

<!-- 
TODO: see if this works for our examples
-->

--
count: false
```cpp
dependent(  1 Load x,   1 Load y    );       // same thread
dependent(  1 Load x,   2 Store x   );       // same operand and one write
!dependent( 1 Load x,   2 Load x    );       // same operand, but no write
!dependent( 1 Load x,   2 Store y   );       // different threads, different operands
```

???
BRIDGE FROM:
Based on the dependence relation a happens-before relation can be defined.

<!-- <<< -->

<!-- >>> Slide -->

---

##### Happens-Before Relation
In an execution E, instruction E[i] *happens-before* E[j] iff
- i < j; and
- E[i] and E[j] are *dependent*

???
i < j: In other words, E[i] is executed before E[j] in E

--
count: false
<div class="animation_no_shift_up" style="margin-top: -10em;">
   .right[<img src='./generated_trees/data_race_fixed_pthread.cpp/bounded_search/0/trees/animations/0.png' height="500"/>]
</div>

<!-- <<< -->

---

<!-- TODO: dependence, happens-before -->

<!-- >>> Slide -->

---

##### :) Complete
Explores at least one schedule per equivalence class

--
count: false
##### :) Ideally non-redundant
Ideally explores exactly one schedule per equivalence class

--
count: false
##### :( No obvious coverage versus resources trade-off 
- For large programs with many dependencies between threads, partial order reduction may still be 
infeasible
- No iterative approach with incomplete but quantifiable coverage guarantees

<!-- <<< -->

<!-- >>> Slide -->

---
layout: true
#### Systematic Exploration - Exploration Algorithms
# Partial Order Reduction
<span class="h_example">background_thread.cpp</span>
---
---
count: false
<div class="animation_dpor" style="margin-top: -7em">
   .right[<img src='./generated_trees/background_thread.cpp/dpor/trees/animations/0.png' height="560"/>]
</div>
---
count: false
<div class="animation_dpor" style="margin-top: -7em">
   .right[<img src='./generated_trees/background_thread.cpp/dpor/trees/animations/1.png' height="560"/>]
</div>
---
count: false
<div class="animation_dpor" style="margin-top: -7em">
   .right[<img src='./generated_trees/background_thread.cpp/dpor/trees/animations/2.png' height="560"/>]
</div>
---

<!-- <<< -->





<!-- section:Outro -->

<!-- >>> Slide -->

---
count: false
layout: false
class: slide_section, no_number
# Thank you!

<!-- <<< -->

---
layout: true
# References

<!-- >>> Slide -->

---

### Tools
<span class="h_link">StateSpaceExplorer:</span>
https://github.com/s-vde/state-space-explorer

<span class="h_link">Helgrind:</span>
http://valgrind.org/docs/manual/hg-manual.html

<span class="h_link">ThreadSanitizer:</span>
https://github.com/google/sanitizers/wiki

<br/><br/>
<span class="h_link">CHESS:</span>
https://chesstool.codeplex.com

<span class="h_link">Maple:</span>
https://github.com/jieyu/maple

<!-- <<< -->

<!-- >>> Slide -->

---

### Selected Literature
C. Flanagan and P. Godefroid (2005),
*Dynamic Partial-Order Reduction for Model Checking Software*,
In: POPL' 05,
pages 110-121
<br class="br_link" />
https://dl.acm.org/citation.cfm?id=1040315

P. Godefroid (1997),
*Model Checking for Programming Languages using VeriSoft*,
InL POPL'97,
pages 174-186
<br class="br_link" />
https://dl.acm.org/citation.cfm?id=263717&CFID=824726696&CFTOKEN=68367255

M. Musuvathi and S. Qadeer (2007),
*Iterative Context Bounding for Systematic Testing of Multithreaded Programs*,
In: PLDI' 07,
pages 446-455
<br class="br_link" />
https://dl.acm.org/citation.cfm?id=1040315

M. Musuvathi et al. (2008), 
*Finding and Reproducing Heisenbugs in Concurrent Programs*,
In OSDI'08,
pages 267-280
<br class="br_link" />
https://dl.acm.org/citation.cfm?id=1855760

<!-- <<< -->

<!-- ////////// -->




      </textarea>
      
      <script src="../libs/mermaid/dist/mermaid.js"></script>
      <link rel="stylesheet" href="../libs/mermaid/dist/mermaid.forest.css">
      <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script>
      <script>
         var slideshow = remark.create({ 
            ratio: '16:9',
            highlightLanguage: 'cpp',
            highlightStyle: 'github',
            highlightLines: true
         });
         mermaid.initialize({ 
            startOnLoad: false,
            cloneCssStyles: false
         });
         function initMermaid(s) {
            var diagrams = document.querySelectorAll('.mermaid');
            var i;
            for (i=0; i < diagrams.length; i++) {
               if(diagrams[i].offsetWidth>0){
                  mermaid.init(undefined, diagrams[i]);
               }
            }
         }
         slideshow.on('afterShowSlide', initMermaid);
         initMermaid(slideshow.getSlides()[slideshow.getCurrentSlideIndex()]);
      </script>
      
   </body>
</html>
